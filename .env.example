DEBUG_MODE=false
# LLM provider config
# Supported providers: openai | deepseek | ollama | openai_compatible
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:8b
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=None

INGESTION_PROMPT_VERSION=ingestion.extract.v11
ENVELOPE_REFINE_PROMPT_VERSION=envelope_refine.v2
CONTEXT_UPDATE_PROMPT_VERSION=context_update.v2
THINKING_PROMPT_VERSION=thinking.v2
THINKING_OUTPUT_DIR=data/thinking_runs
THINKING_MAX_CARDS=200
THINKING_MAX_ENVELOPES=100

# Embeddings config
# EMBEDDING_PROVIDER: auto | lexical | openai | deepseek | ollama | openai_compatible
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=qwen3-embedding:0.6b
EMBEDDING_API_KEY=None
EMBEDDING_BASE_URL=http://localhost:11434/v1

DATABASE_URL=sqlite:///assistant-demo.db
TIMEZONE=UTC
ENVELOPE_ASSIGN_THRESHOLD=0.4
EMBEDDING_WEIGHT=0.6
KEYWORD_WEIGHT=0.3
ENTITY_WEIGHT=0.1
