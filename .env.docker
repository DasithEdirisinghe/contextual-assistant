DEBUG_MODE=false

# LLM provider config
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:8b
LLM_BASE_URL=http://host.docker.internal:11434/v1
LLM_API_KEY=None

INGESTION_PROMPT_VERSION=ingestion.extract.v10
ENVELOPE_REFINE_PROMPT_VERSION=envelope_refine.v1
CONTEXT_UPDATE_PROMPT_VERSION=context_update.v1
THINKING_PROMPT_VERSION=thinking.v1
THINKING_OUTPUT_DIR=data/thinking_runs
THINKING_MAX_CARDS=200
THINKING_MAX_ENVELOPES=100

# Embedding config
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=qwen3-embedding:0.6b
EMBEDDING_API_KEY=None
EMBEDDING_BASE_URL=http://host.docker.internal:11434/v1

DATABASE_URL=sqlite:///data/assistant-demo.db
TIMEZONE=UTC
ENVELOPE_ASSIGN_THRESHOLD=0.4
EMBEDDING_WEIGHT=0.6
KEYWORD_WEIGHT=0.3
ENTITY_WEIGHT=0.1
